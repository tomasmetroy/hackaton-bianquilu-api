{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key = config['openai']['api_key']\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to interact with the ChatGPT model\n",
    "def chat_with_model(message):\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=message,\n",
    "        max_tokens=50,\n",
    "        temperature=0.7,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=10\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_model(\"Hello, I'm a robot\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"\"\"\n",
    "Vota mucho gas y emana olores, la compre pensando que iba a ser un ahorro en la luz, pero fue todo lo contrario, consumio un balon de gas premiun en una semana. Encima la valvula vino desconpuesta por dentro, al agitar se escuchaba que tenia algo adentro, tampoco ajustaba bien al balon de gas, quedaba muy suelto. Tuve que devolverla.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\" \n",
    "Según el comentario, cuáles son las principales características del producto?\n",
    "\n",
    "Comentario: {comment}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, \n",
    "                        input_variables=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSegún el comentario, cuáles son las principales características del producto?\\n\\nComentario: \\nVota mucho gas y emana olores, la compre pensando que iba a ser un ahorro en la luz, pero fue todo lo contrario, consumio un balon de gas premiun en una semana. Encima la valvula vino desconpuesta por dentro, al agitar se escuchaba que tenia algo adentro, tampoco ajustaba bien al balon de gas, quedaba muy suelto. Tuve que devolverla.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Las principales características del producto son: consumo excesivo de gas, emisión de olores, válvula defectuosa y ajuste inadecuado al balón de gas.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "examples = [{\"comentario\": \"La pelota es muy liviana\", \"palabras clave\": [\"liviana\"]},\n",
    "            {\"comentario\": \"El jabón tiene un olor exquisito\", \"palabras clave\": [\"fragancia exquisita\"]}]\n",
    "\n",
    "example_formatter = \"\"\"\n",
    "Comentario: {comentario}\n",
    "Palabras clave: {palabras clave}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['comentario', 'palabras clave'],\n",
    "                                template=example_formatter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(examples = examples,\n",
    "                                        example_prompt = example_prompt,\n",
    "                                        prefix= \"Según el comentario, cuáles son las principales características del producto?\\n\",\n",
    "                                        suffix= \"\\n\\nComentario: {input}\\nPalabras clave:\",\n",
    "                                        input_variables=['input'],\n",
    "                                        example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según el comentario, cuáles son las principales características del producto?\n",
      "\n",
      "\n",
      "\n",
      "Comentario: La pelota es muy liviana\n",
      "Palabras clave: liviana\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Comentario: El jabón tiene un olor exquisito\n",
      "Palabras clave: fragancia exquisita\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Comentario: \n",
      "Vota mucho gas y emana olores, la compre pensando que iba a ser un ahorro en la luz, pero fue todo lo contrario, consumio un balon de gas premiun en una semana. Encima la valvula vino desconpuesta por dentro, al agitar se escuchaba que tenia algo adentro, tampoco ajustaba bien al balon de gas, quedaba muy suelto. Tuve que devolverla.\n",
      "\n",
      "Palabras clave:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format(input=comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Consumo de gas alto, valvula descompuesta, ajuste suelto.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "print(chain.run(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "\n",
    "full_chain = SequentialChain(chains = [chain, chain, chain],verbose=True)\n",
    "\n",
    "print(full_chain.run(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat llm chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "\n",
    "prefix_message = [{\"role\":\"system\",\"content\":\"You are a helpful assistant\"}]\n",
    "\n",
    "\n",
    "llm = OpenAIChat(model_name='gpt-3.5-turbp', temperature=0.3, prefix_message=prefix_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next line is reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, overlap_size=200)\n",
    "#texts = text_splitter.split(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json') as f:\n",
    "    sample_response = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_text(product):\n",
    "    text = f\"\"\"\n",
    "    title: {product['title']}\n",
    "    description: {product['description']}\n",
    "    specs: {\", \".join([x + \" \" + y for x,y in product['specs'].items()])}\n",
    "    \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [product_text(product) for product in sample_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db2'\n",
    "\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_texts(texts=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_aspiradora = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n    title: Aspiradora xd\\n    description: aspiradora de mano\\n    specs: Corriente 5A, color rojo\\n    ', metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"aspiradora barata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydayenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
